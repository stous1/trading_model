{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      "org.apache.kudu#kudu-spark3_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a2871d72-0818-4f44-b855-09ca5a12eb14;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;3.4.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.4.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.kudu#kudu-spark3_2.12;1.17.0 in central\n",
      ":: resolution report :: resolve 742ms :: artifacts dl 21ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.kudu#kudu-spark3_2.12;1.17.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;3.4.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a2871d72-0818-4f44-b855-09ca5a12eb14\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/12ms)\n",
      "2024-10-30 02:36:00,278 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "class raw_kafka_to_kudu:\n",
    "    def __init__(self, sparkMaster, appName, params):\n",
    "        # base params\n",
    "        self.appName = appName\n",
    "        self.sparkMaster = sparkMaster\n",
    "        \n",
    "        # extra params\n",
    "        self.kafkaBroker = params.get('kafkaBroker')\n",
    "        self.kuduMaster = params.get('kuduMaster')\n",
    "        \n",
    "        # execute process\n",
    "        self.ejecutar()\n",
    "        \n",
    "    def ejecutar(self):\n",
    "        # \n",
    "        sparkConn = self.createSparkSession()\n",
    "        \n",
    "        # \n",
    "        self.avgPriceProcess(sparkConn)\n",
    "        \n",
    "        #\n",
    "        self.pricesKlinesProcess(sparkConn)\n",
    "        \n",
    "        #       \n",
    "        self.tickerProcess(sparkConn)\n",
    "        \n",
    "        # \n",
    "        self.tradesProcess(sparkConn)\n",
    "    \n",
    "    def avgPriceProcess(self, sparkConn):\n",
    "        # \n",
    "        topicName = 'pairs_avg_price'\n",
    "        raw_df = self.readFromKafka(sparkConn, topicName)\n",
    "        \n",
    "        # \n",
    "        outputFields, schema = self.schAvgPrices()\n",
    "\n",
    "        # \n",
    "        json_df = raw_df.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "                .select(from_json(col(\"json\"), schema).alias(\"data\")) \\\n",
    "                .select(\"data.*\")\n",
    "        \n",
    "        # \n",
    "        final_df = json_df.toDF(*outputFields)\n",
    "        \n",
    "        # \n",
    "        kuduTableName = 'impala::s_productos.k_pairs_avg_prices'\n",
    "        (final_df.writeStream \\\n",
    "                .queryName(topicName) \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .foreachBatch(lambda batch_df, batch_id: self.writeToKudu(batch_df, batch_id, kuduTableName)) \\\n",
    "                .option(\"checkpointLocation\", f\"hdfs://namenode:9000/tmp/pyspark/checkpoints/{topicName}\") \\\n",
    "                .start())\n",
    "        \n",
    "        # \n",
    "        # (query.awaitTermination())\n",
    "    \n",
    "    def pricesKlinesProcess(self, sparkConn):\n",
    "        # \n",
    "        topicName = 'pairs_prices_klines'\n",
    "        raw_df = self.readFromKafka(sparkConn, topicName)\n",
    "        \n",
    "        # \n",
    "        first_column_names, final_column_names, schema = self.schPricesKlines()\n",
    "\n",
    "        # \n",
    "        json_df = raw_df.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "                .select(from_json(col(\"json\"), schema).alias(\"data\")) \\\n",
    "                .select(\"data.*\")\n",
    "                \n",
    "        # Renombrar las columnas usando toDF\n",
    "        renamed_df = json_df.toDF(*first_column_names)\n",
    "        \n",
    "        # \n",
    "        df_flattened = renamed_df.select(\n",
    "            \"event_type\",\n",
    "            \"event_timestamp\",\n",
    "            \"currency\",\n",
    "            \"kline.*\"\n",
    "        )\n",
    "\n",
    "        # \n",
    "        final_df = df_flattened.toDF(*final_column_names)\n",
    "        \n",
    "        # \n",
    "        kuduTableName = 'impala::s_productos.k_klines'\n",
    "        (final_df.writeStream \\\n",
    "                .queryName(topicName) \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .foreachBatch(lambda batch_df, batch_id: self.writeToKudu(batch_df, batch_id, kuduTableName)) \\\n",
    "                .option(\"checkpointLocation\", f\"hdfs://namenode:9000/tmp/pyspark/checkpoints/{topicName}\") \\\n",
    "                .start())\n",
    "        \n",
    "        # \n",
    "        # (query.awaitTermination())\n",
    "    \n",
    "    def tickerProcess(self, sparkConn):\n",
    "        # \n",
    "        topicName = 'pairs_ticker'\n",
    "        raw_df = self.readFromKafka(sparkConn, topicName)\n",
    "        \n",
    "        # \n",
    "        outputFields, schema = self.sch24hrTicker()\n",
    "\n",
    "        # \n",
    "        json_df = raw_df.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "                .select(from_json(col(\"json\"), schema).alias(\"data\")) \\\n",
    "                .select(\"data.*\")\n",
    "        \n",
    "        # \n",
    "        final_df = json_df.toDF(*outputFields)\n",
    "        \n",
    "        # \n",
    "        kuduTableName = 'impala::s_productos.k_ticker'\n",
    "        (final_df.writeStream \\\n",
    "                .queryName(topicName) \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .foreachBatch(lambda batch_df, batch_id: self.writeToKudu(batch_df, batch_id, kuduTableName)) \\\n",
    "                .option(\"checkpointLocation\", f\"hdfs://namenode:9000/tmp/pyspark/checkpoints/{topicName}\") \\\n",
    "                .start())\n",
    "        \n",
    "        # \n",
    "        # (query.awaitTermination())\n",
    "    \n",
    "    def tradesProcess(self, sparkConn):\n",
    "        # \n",
    "        topicName = 'pairs_trades'\n",
    "        raw_df = self.readFromKafka(sparkConn, topicName)\n",
    "        \n",
    "        # \n",
    "        outputFields, schema = self.schTrades()\n",
    "\n",
    "        # \n",
    "        json_df = raw_df.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "                .select(from_json(col(\"json\"), schema).alias(\"data\")) \\\n",
    "                .select(\"data.*\")\n",
    "        \n",
    "        # \n",
    "        final_df = json_df.toDF(*outputFields)\n",
    "        \n",
    "        # \n",
    "        kuduTableName = 'impala::s_productos.k_trades'\n",
    "        (final_df.writeStream \\\n",
    "                .queryName(topicName) \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .foreachBatch(lambda batch_df, batch_id: self.writeToKudu(batch_df, batch_id, kuduTableName)) \\\n",
    "                .option(\"checkpointLocation\", f\"hdfs://namenode:9000/tmp/pyspark/checkpoints/{topicName}\") \\\n",
    "                .start())\n",
    "        \n",
    "        # \n",
    "        # (query.awaitTermination())\n",
    "    \n",
    "    def schAvgPrices(self):\n",
    "        field_names = [\"event_type\", \"event_timestamp\", \"currency\", \"time_interval\", \"avg_price\", \"time_stamp\"]\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"e\", StringType(), True),\n",
    "            StructField(\"E\", LongType(), True),\n",
    "            StructField(\"s\", StringType(), True),\n",
    "            StructField(\"i\", StringType(), True),\n",
    "            StructField(\"w\", StringType(), True),\n",
    "            StructField(\"T\", LongType(), True)\n",
    "        ])\n",
    "        \n",
    "        return field_names, schema\n",
    "    \n",
    "    def schPricesKlines(self):\n",
    "        start_field_names = [\"event_type\", \"event_timestamp\", \"currency\", \"kline\"]\n",
    "        \n",
    "        final_field_names = [\n",
    "            \"event_type\", \"event_timestamp\", \"currency\", \"kline_start_time\", \"kline_close_time\",\n",
    "            \"kline_symbol\", \"time_interval\", \"kline_first_trade_id\", \"kline_last_trade_id\",\n",
    "            \"kline_open_price\", \"kline_close_price\", \"kline_high_price\", \"kline_low_price\",\n",
    "            \"kline_volume\", \"kline_trade_count\", \"kline_is_closed\", \"kline_quote_asset_volume\",\n",
    "            \"kline_active_buy_volume\", \"kline_active_buy_quote_volume\", \"kline_ignore\"\n",
    "        ]\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"e\", StringType(), True),\n",
    "            StructField(\"E\", LongType(), True),\n",
    "            StructField(\"s\", StringType(), True),\n",
    "            StructField(\"k\", StructType([\n",
    "                StructField(\"t\", LongType(), True),\n",
    "                StructField(\"T\", LongType(), True),\n",
    "                StructField(\"s\", StringType(), True),\n",
    "                StructField(\"i\", StringType(), True),\n",
    "                StructField(\"f\", LongType(), True),\n",
    "                StructField(\"L\", LongType(), True),\n",
    "                StructField(\"o\", StringType(), True),\n",
    "                StructField(\"c\", StringType(), True),\n",
    "                StructField(\"h\", StringType(), True),\n",
    "                StructField(\"l\", StringType(), True),\n",
    "                StructField(\"v\", StringType(), True),\n",
    "                StructField(\"n\", IntegerType(), True),\n",
    "                StructField(\"x\", BooleanType(), True),\n",
    "                StructField(\"q\", StringType(), True),\n",
    "                StructField(\"V\", StringType(), True),\n",
    "                StructField(\"Q\", StringType(), True),\n",
    "                StructField(\"B\", StringType(), True)\n",
    "            ]))\n",
    "        ])\n",
    "        \n",
    "        return start_field_names, final_field_names, schema\n",
    "    \n",
    "    def sch24hrTicker(self):\n",
    "        field_names = [\n",
    "            \"event_type\", \"event_timestamp\", \"currency\", \"price_change\", \"price_change_percent\",\n",
    "            \"weighted_avg_price\", \"last_price\", \"last_qty\", \"quote_qty\",  # Agregamos quote_qty\n",
    "            \"bid_price\", \"bid_qty\", \"ask_price\", \"ask_qty\", \"open_price\",\n",
    "            \"high_price\", \"low_price\", \"volume\", \"quote_volume\", \"open_time\",\n",
    "            \"close_time\", \"first_trade_id\", \"last_trade_id\", \"trade_count\"\n",
    "        ]\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"e\", StringType(), True),   # event_type\n",
    "            StructField(\"E\", LongType(), True),      # event_timestamp\n",
    "            StructField(\"s\", StringType(), True),     # currency\n",
    "            StructField(\"p\", StringType(), True),     # price_change\n",
    "            StructField(\"P\", StringType(), True),     # price_change_percent\n",
    "            StructField(\"w\", StringType(), True),     # weighted_avg_price\n",
    "            StructField(\"x\", StringType(), True),     # last_price\n",
    "            StructField(\"c\", StringType(), True),     # last_qty\n",
    "            StructField(\"Q\", StringType(), True),     # quote_qty\n",
    "            StructField(\"b\", StringType(), True),     # bid_price\n",
    "            StructField(\"B\", StringType(), True),     # bid_qty\n",
    "            StructField(\"a\", StringType(), True),     # ask_price\n",
    "            StructField(\"A\", StringType(), True),     # ask_qty\n",
    "            StructField(\"o\", StringType(), True),     # open_price\n",
    "            StructField(\"h\", StringType(), True),     # high_price\n",
    "            StructField(\"l\", StringType(), True),     # low_price\n",
    "            StructField(\"v\", StringType(), True),     # volume\n",
    "            StructField(\"q\", StringType(), True),     # quote_volume\n",
    "            StructField(\"O\", LongType(), True),       # open_time\n",
    "            StructField(\"C\", LongType(), True),       # close_time\n",
    "            StructField(\"F\", LongType(), True),       # first_trade_id\n",
    "            StructField(\"L\", LongType(), True),       # last_trade_id\n",
    "            StructField(\"n\", LongType(), True)        # trade_count\n",
    "        ])\n",
    "        \n",
    "        return field_names, schema\n",
    "\n",
    "    def schTrades(self):\n",
    "        field_names = [\"event_type\", \"event_timestamp\", \"currency\", \"trade_id\", \"price\", \"quantity\", \"trade_time\", \"is_market_maker\", \"is_maker\"]\n",
    "\n",
    "        schema = StructType([\n",
    "            StructField(\"e\", StringType(), True),    # event type\n",
    "            StructField(\"E\", LongType(), True),       # event timestamp\n",
    "            StructField(\"s\", StringType(), True),     # symbol\n",
    "            StructField(\"t\", LongType(), True),       # trade ID\n",
    "            StructField(\"p\", StringType(), True),     # price\n",
    "            StructField(\"q\", StringType(), True),     # quantity\n",
    "            StructField(\"T\", LongType(), True),       # trade time\n",
    "            StructField(\"m\", BooleanType(), True),    # is market maker\n",
    "            StructField(\"M\", BooleanType(), True)     # is maker\n",
    "        ])\n",
    "\n",
    "        return field_names, schema\n",
    "    \n",
    "    def createSparkSession(self):\n",
    "        s_conn = None\n",
    "        try:\n",
    "            # \n",
    "            s_conn = SparkSession.builder \\\n",
    "                .appName(self.appName) \\\n",
    "                .master(self.sparkMaster) \\\n",
    "                .config('spark.executor.memory', '1g') \\\n",
    "                .config('spark.driver.memory', '1g') \\\n",
    "                .config(\"spark.executor.cores\", \"1\") \\\n",
    "                .config(\"spark.cores.max\", \"3\") \\\n",
    "                .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.4.1,org.apache.kudu:kudu-spark3_2.12:1.17.0\") \\\n",
    "                .getOrCreate()\n",
    "\n",
    "            # \n",
    "            s_conn.sparkContext.setLogLevel(\"ERROR\")\n",
    "            logging.info(\"Spark connection created successfully!\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Couldn't create the spark session due to exception {e}\")\n",
    "\n",
    "        return s_conn\n",
    "    \n",
    "    def readFromKafka(self, sparkConn, topicName):\n",
    "        # Leer datos desde el tópico de Kafka\n",
    "        df = sparkConn.readStream  \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", self.kafkaBroker) \\\n",
    "            .option(\"subscribe\", topicName) \\\n",
    "            .option(\"maxOffsetsPerTrigger\", 1000) \\\n",
    "            .option(\"startingOffsets\", \"earliest\") \\\n",
    "            .option(\"failOnDataLoss\", \"false\") \\\n",
    "            .load()\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def writeToKudu(self, batch_df, batch_id, kuduTableName):\n",
    "        # \n",
    "        batch_df.write \\\n",
    "            .format(\"kudu\") \\\n",
    "            .option(\"kudu.master\", self.kuduMaster) \\\n",
    "            .option(\"kudu.table\", kuduTableName) \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # base params\n",
    "    sparkMaster = 'spark://e5616452f8d5:7077'\n",
    "    appName = 'KafkaToKuduRaw1'\n",
    "    \n",
    "    # extra params\n",
    "    extraParams = None\n",
    "    extraParams = {\n",
    "            'kafkaBroker' : 'kafka-broker:29092',\n",
    "            'kuduMaster' : 'kudu-master-1:7051,kudu-master-2:7151,kudu-master-3:7251'\n",
    "            }\n",
    "    \n",
    "    app = raw_kafka_to_kudu(sparkMaster, appName, params=extraParams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark_master = \"spark://e5616452f8d5:7077\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"prueba_kafka\") \\\n",
    "        .master(spark_master) \\\n",
    "        .config('spark.executor.memory', '1g') \\\n",
    "        .config('spark.driver.memory', '1g') \\\n",
    "        .config(\"spark.executor.cores\", \"1\") \\\n",
    "        .config(\"spark.cores.max\", \"2\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.4.1,org.apache.kudu:kudu-spark3_2.12:1.17.0\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schAvgPrices():\n",
    "    field_names = [\"event_type\", \"event_timestamp\", \"currency\", \"time_interval\", \"avg_price\", \"time_stamp\"]\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"e\", StringType(), True),\n",
    "        StructField(\"E\", LongType(), True),\n",
    "        StructField(\"s\", StringType(), True),\n",
    "        StructField(\"i\", StringType(), True),\n",
    "        StructField(\"w\", StringType(), True),\n",
    "        StructField(\"T\", LongType(), True)\n",
    "    ])\n",
    "    \n",
    "    return field_names, schema\n",
    "\n",
    "def schPricesKlines():\n",
    "    start_field_names = [\"event_type\", \"event_timestamp\", \"currency\", \"kline\"]\n",
    "    \n",
    "    final_field_names = [\n",
    "        \"event_type\", \"event_timestamp\", \"currency\", \"kline_start_time\", \"kline_close_time\",\n",
    "        \"kline_symbol\", \"time_interval\", \"kline_first_trade_id\", \"kline_last_trade_id\",\n",
    "        \"kline_open_price\", \"kline_close_price\", \"kline_high_price\", \"kline_low_price\",\n",
    "        \"kline_volume\", \"kline_trade_count\", \"kline_is_closed\", \"kline_quote_asset_volume\",\n",
    "        \"kline_active_buy_volume\", \"kline_active_buy_quote_volume\", \"kline_ignore\"\n",
    "    ]\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"e\", StringType(), True),\n",
    "        StructField(\"E\", LongType(), True),\n",
    "        StructField(\"s\", StringType(), True),\n",
    "        StructField(\"k\", StructType([\n",
    "            StructField(\"t\", LongType(), True),\n",
    "            StructField(\"T\", LongType(), True),\n",
    "            StructField(\"s\", StringType(), True),\n",
    "            StructField(\"i\", StringType(), True),\n",
    "            StructField(\"f\", LongType(), True),\n",
    "            StructField(\"L\", LongType(), True),\n",
    "            StructField(\"o\", StringType(), True),\n",
    "            StructField(\"c\", StringType(), True),\n",
    "            StructField(\"h\", StringType(), True),\n",
    "            StructField(\"l\", StringType(), True),\n",
    "            StructField(\"v\", StringType(), True),\n",
    "            StructField(\"n\", IntegerType(), True),\n",
    "            StructField(\"x\", BooleanType(), True),\n",
    "            StructField(\"q\", StringType(), True),\n",
    "            StructField(\"V\", StringType(), True),\n",
    "            StructField(\"Q\", StringType(), True),\n",
    "            StructField(\"B\", StringType(), True)\n",
    "        ]))\n",
    "    ])\n",
    "    \n",
    "    return start_field_names, final_field_names, schema\n",
    "\n",
    "def sch24hrTicker():\n",
    "    field_names = [\n",
    "        \"event_type\", \"event_timestamp\", \"currency\", \"price_change\", \"price_change_percent\",\n",
    "        \"weighted_avg_price\", \"last_price\", \"last_qty\", \"quote_qty\",  # Agregamos quote_qty\n",
    "        \"bid_price\", \"bid_qty\", \"ask_price\", \"ask_qty\", \"open_price\",\n",
    "        \"high_price\", \"low_price\", \"volume\", \"quote_volume\", \"open_time\",\n",
    "        \"close_time\", \"first_trade_id\", \"last_trade_id\", \"trade_count\"\n",
    "    ]\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"e\", StringType(), True),   # event_type\n",
    "        StructField(\"E\", LongType(), True),      # event_timestamp\n",
    "        StructField(\"s\", StringType(), True),     # currency\n",
    "        StructField(\"p\", StringType(), True),     # price_change\n",
    "        StructField(\"P\", StringType(), True),     # price_change_percent\n",
    "        StructField(\"w\", StringType(), True),     # weighted_avg_price\n",
    "        StructField(\"x\", StringType(), True),     # last_price\n",
    "        StructField(\"c\", StringType(), True),     # last_qty\n",
    "        StructField(\"Q\", StringType(), True),     # quote_qty\n",
    "        StructField(\"b\", StringType(), True),     # bid_price\n",
    "        StructField(\"B\", StringType(), True),     # bid_qty\n",
    "        StructField(\"a\", StringType(), True),     # ask_price\n",
    "        StructField(\"A\", StringType(), True),     # ask_qty\n",
    "        StructField(\"o\", StringType(), True),     # open_price\n",
    "        StructField(\"h\", StringType(), True),     # high_price\n",
    "        StructField(\"l\", StringType(), True),     # low_price\n",
    "        StructField(\"v\", StringType(), True),     # volume\n",
    "        StructField(\"q\", StringType(), True),     # quote_volume\n",
    "        StructField(\"O\", LongType(), True),       # open_time\n",
    "        StructField(\"C\", LongType(), True),       # close_time\n",
    "        StructField(\"F\", LongType(), True),       # first_trade_id\n",
    "        StructField(\"L\", LongType(), True),       # last_trade_id\n",
    "        StructField(\"n\", LongType(), True)        # trade_count\n",
    "    ])\n",
    "    \n",
    "    return field_names, schema\n",
    "\n",
    "def schTrades():\n",
    "    field_names = [\"event_type\", \"event_timestamp\", \"currency\", \"trade_id\", \"price\", \"quantity\", \"trade_time\", \"is_market_maker\", \"is_maker\"]\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"e\", StringType(), True),    # event type\n",
    "        StructField(\"E\", LongType(), True),       # event timestamp\n",
    "        StructField(\"s\", StringType(), True),     # symbol\n",
    "        StructField(\"t\", LongType(), True),       # trade ID\n",
    "        StructField(\"p\", StringType(), True),     # price\n",
    "        StructField(\"q\", StringType(), True),     # quantity\n",
    "        StructField(\"T\", LongType(), True),       # trade time\n",
    "        StructField(\"m\", BooleanType(), True),    # is market maker\n",
    "        StructField(\"M\", BooleanType(), True)     # is maker\n",
    "    ])\n",
    "\n",
    "    return field_names, schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topics\n",
    "# Topic name\n",
    "# ####\n",
    "# pairs_avg_price -- OK \n",
    "# pairs_prices_klines -- OK\n",
    "# pairs_ticker --OK\n",
    "# pairs_trades -- OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre del tópico de Kafka\n",
    "kafka_topic = \"pairs_ticker\"\n",
    "\n",
    "# Leer datos desde el tópico de Kafka\n",
    "df = spark.readStream  \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka-broker:29092\") \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 300) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_column_names, schema = sch24hrTicker()\n",
    "\n",
    "json_df = df.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "    .select(from_json(col(\"json\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# df_flattened = json_df.select(\n",
    "#     \"event_type\",\n",
    "#     \"event_timestamp\",\n",
    "#     \"currency\",\n",
    "#     \"kline.*\"   # Volumen base de transacción\n",
    "# )\n",
    "\n",
    "renamed_df = json_df.toDF(*final_column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renamed_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para escribir en Kudu\n",
    "def write_to_kudu(batch_df, batch_id):\n",
    "    # Especificar la tabla Kudu a la que deseas insertar\n",
    "    kudu_table = \"impala::s_productos.k_ticker\"\n",
    "    kudu_masters = \"kudu-master-1:7051,kudu-master-2:7151,kudu-master-3:7251\"\n",
    "    \n",
    "    batch_df.write \\\n",
    "        .format(\"kudu\") \\\n",
    "        .option(\"kudu.master\", kudu_masters) \\\n",
    "        .option(\"kudu.table\", kudu_table) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar foreachBatch para insertar los datos en Kudu\n",
    "query = renamed_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(write_to_kudu) \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://namenode:9000/tmp/pyspark/checkpoints/pairs_ticker\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esperar a que la consulta termine\n",
    "(query.awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
